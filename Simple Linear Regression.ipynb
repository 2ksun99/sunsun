{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data는 입력, y_data는 출력으로 입력을 그대로 출력해주는 모델\n",
    "# x값과 y값이 같은 이유는 hypothesis의 w = 1, b = 0 인것을 확인하기 위함이다.\n",
    "import tensorflow as tf \n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1,2,3,4,5]\n",
    "y_data = [1,2,3,4,5]\n",
    "# w와 b를 각각 2.9와 0.5로 초기값을 지정한다. 이 후에는 랜덤 값으로도 지정할 수 있다.\n",
    "w = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "\n",
    "# hypothesis = w * x + b 우리의 가설함수다.\n",
    "hypothesis = w * x_data + b\n",
    "\n",
    "# cost는 hypothesis 값에서 y값을 빼고 제곱한것의 평균을 구한값이다. \n",
    "# tf.reduce_mean는 차원이 줄어들면서 평균을 구하는것이다. tf.square는 제곱을 나타낸다.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0|    1.0048|  -0.01727|  0.000055\n",
      "   10|    1.0046|   -0.0167|  0.000051\n",
      "   20|    1.0045|  -0.01614|  0.000048\n",
      "   30|    1.0043|   -0.0156|  0.000045\n",
      "   40|    1.0042|  -0.01508|  0.000042\n",
      "   50|    1.0040|  -0.01458|  0.000039\n",
      "   60|    1.0039|   -0.0141|  0.000036\n",
      "   70|    1.0038|  -0.01363|  0.000034\n",
      "   80|    1.0036|  -0.01317|  0.000032\n",
      "   90|    1.0035|  -0.01273|  0.000030\n",
      "  100|    1.0034|  -0.01231|  0.000028\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "# 경사 하강법 (Gradient descent)는 cost를 최소화하는 w와 b를 찾는 알고리즘이다.\n",
    "for i in range(100+1): # for문을 이용해 Gradient descent를 101번 수행한다.\n",
    "    with tf.GradientTape() as tape: # tf.GradientTape은 with와 함께 쓰인다.\n",
    "        hypothesis = w * x_data + b # 변수들의 정보인 w와 b를 위 tape에 기록한다. \n",
    "        cost = tf.reduce_mean(tf.square(hypothesis-y_data))\n",
    "    w_grad, b_grad = tape.gradient(cost, [w, b])\n",
    "    # tape.gradient를 이용해 w, b의 경사도 값을 w_grad, b_grad에 반환한다.\n",
    "    \n",
    "    # A.assign_sub(B)\n",
    "    # (A = A - B, A -= B)를 이용하여 w와 b를 각각 업데이트한다.\n",
    "    w.assign_sub(learning_rate * w_grad) # learining_rate은 gradient값을 얼마만큼 반영할지 결정한다.\n",
    "    b.assign_sub(learning_rate * b_grad) # learining_rate가 크면 많이 반영하며, 작으면 조금씩 변화가 일어난다. \n",
    "    if i % 10 == 0:\n",
    "        print(\"{:5}|{:10.4f}|{:10.4}|{:10.6f}\".format(i, w.numpy(),b.numpy(),cost))\n",
    "        # w, b, cost값들이 중간에 변하는것을 확인하기 위해 i가 10의 배수가 될때 마다 확인을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
